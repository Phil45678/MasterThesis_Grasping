{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56307e5",
   "metadata": {},
   "source": [
    "# Loading and preprocessing, saving YCB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c37cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "\n",
    "nb_classes = 16\n",
    "rand_seed = 42\n",
    "one_hot = True\n",
    "test_train_ratio = 0.2\n",
    "verbose = 2\n",
    "seq_len = 10\n",
    "nb_samples = 100000\n",
    "joint_dist_lim = 0.005\n",
    "nb_tot_samp_class = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc57bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set location \n",
    "data_set_loc_str = r\"C:\\Users\\phili\\Documents\\GitHub\\DexterousManipulation\\generations\\DATA_SET_YCB_filtered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96665574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data saving string \n",
    "prefix_str = r\"C:\\Users\\phili\\Documents\\GitHub\\DexterousManipulation\\generations\\Preprocessed_YCB\"\n",
    "file_nb_int = int(nb_tot_samp_class/1000)\n",
    "data_save_str_train = prefix_str + \"/\" + str(file_nb_int) + \"K_per_class/seq_len_\" + str(seq_len) + \"/YCB_train.npz\"\n",
    "data_save_str_test = prefix_str + \"/\" + str(file_nb_int) + \"K_per_class/seq_len_\" + str(seq_len) + \"/YCB_test.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2deb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import tensorflow as tf \n",
    "import time \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6b99d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_non_zero_fused_grasps(data_set_loc_str, nb_classes, lim):\n",
    "\n",
    "    x_min = -0.15\n",
    "    x_max = 0.15\n",
    "    y_min = -0.15\n",
    "    y_max = 0.15 \n",
    "    z_min = 0.13\n",
    "    z_max = 0.35\n",
    "    gripper_min = 0 \n",
    "    gripper_max = 0.041664\n",
    "     \n",
    "    non_zero_grasps_list = []\n",
    "    for index in range(nb_classes):\n",
    "        non_zero_grasps_list.append([])\n",
    "        \n",
    "    for filename in os.listdir(data_set_loc_str): \n",
    "        file_data = np.load(data_set_loc_str + \"/\" + filename)\n",
    "        \n",
    "        file_metrics = file_data[\"metric\"].astype(np.float64)\n",
    "        file_hand_info = file_data[\"hand\"].astype(np.float64)\n",
    "        file_obj_classes = file_data[\"obj\"].astype(np.float64)\n",
    "        \n",
    "        for metric_index in range(len(file_metrics)):\n",
    "            if file_metrics[metric_index, 0] == 1.0 and file_metrics[metric_index,1] > lim:\n",
    "                curr_class = int(file_obj_classes[metric_index,0])\n",
    "                tmp_list = [] \n",
    "                \n",
    "                tmp_list.append((file_metrics[metric_index,1] - gripper_min) / (gripper_max - gripper_min))\n",
    "                tmp_hand_info = file_hand_info[metric_index]\n",
    "                tmp_hand_info[0] = (tmp_hand_info[0] - x_min) / (x_max - x_min)\n",
    "                tmp_hand_info[1] = (tmp_hand_info[1] - y_min) / (y_max - y_min)\n",
    "                tmp_hand_info[2] = (tmp_hand_info[2] - z_min) / (z_max - z_min)\n",
    "                \n",
    "                for tmp_index in range(13): \n",
    "                    tmp_list.append(tmp_hand_info[tmp_index])\n",
    "                \n",
    "                non_zero_grasps_list[curr_class].append(tmp_list.copy())\n",
    "                \n",
    "    return non_zero_grasps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23b18c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zero_fused_grasps_buckets(data_set_loc_str, nb_classes, lim, nb_tot_samp_class): \n",
    "    x_min = -0.15\n",
    "    x_max = 0.15\n",
    "    y_min = -0.15\n",
    "    y_max = 0.15 \n",
    "    z_min = 0.13\n",
    "    z_max = 0.35\n",
    "    gripper_min = 0 \n",
    "    gripper_max = 0.041664\n",
    "     \n",
    "    zero_grasps_list = []\n",
    "    for index in range(nb_classes):\n",
    "        zero_grasps_list.append([])\n",
    "        \n",
    "    for filename in os.listdir(data_set_loc_str): \n",
    "        file_data = np.load(data_set_loc_str + \"/\" + filename)\n",
    "        \n",
    "        file_metrics = file_data[\"metric\"].astype(np.float64)\n",
    "        file_hand_info = file_data[\"hand\"].astype(np.float64)\n",
    "        file_obj_classes = file_data[\"obj\"].astype(np.float64)\n",
    "        \n",
    "        counters = [] \n",
    "        for tmp_index in range(nb_classes): \n",
    "            counters.append(len(zero_grasps_list[tmp_index]))\n",
    "            \n",
    "        enough_samples = True \n",
    "        for tmp_index in range(nb_classes): \n",
    "            if counters[tmp_index] < nb_tot_samp_class:\n",
    "                # not enough samples \n",
    "                enough_samples = False \n",
    "        if enough_samples: \n",
    "            break \n",
    "        \n",
    "        for metric_index in range(len(file_metrics)):\n",
    "            if file_metrics[metric_index, 0] == 1.0 and file_metrics[metric_index,1] < lim:\n",
    "                curr_class = int(file_obj_classes[metric_index,0])\n",
    "                tmp_list = [] \n",
    "                \n",
    "                tmp_list.append((file_metrics[metric_index,1] - gripper_min) / (gripper_max - gripper_min))\n",
    "                tmp_hand_info = file_hand_info[metric_index]\n",
    "                tmp_hand_info[0] = (tmp_hand_info[0] - x_min) / (x_max - x_min)\n",
    "                tmp_hand_info[1] = (tmp_hand_info[1] - y_min) / (y_max - y_min)\n",
    "                tmp_hand_info[2] = (tmp_hand_info[2] - z_min) / (z_max - z_min)\n",
    "                \n",
    "                for tmp_index in range(13): \n",
    "                    tmp_list.append(tmp_hand_info[tmp_index])\n",
    "                \n",
    "                zero_grasps_list[curr_class].append(tmp_list.copy())\n",
    "                \n",
    "    return zero_grasps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c33cbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_class_buckets(non_zero_grasps_list, zero_grasps_list, nb_tot_samp_class, nb_classes):\n",
    "    equalized_grasps = non_zero_grasps_list.copy()\n",
    "    for class_index in range(nb_classes): \n",
    "        curr_len = len(non_zero_grasps_list[class_index])\n",
    "        samp_to_complete = nb_tot_samp_class - curr_len\n",
    "        # Clip, if necessary \n",
    "        if samp_to_complete < 0: \n",
    "            equalized_grasps[class_index] = equalized_grasps[class_index][:nb_tot_samp_class]\n",
    "        else: \n",
    "            for samp_index in range(samp_to_complete): \n",
    "                equalized_grasps[class_index].append(zero_grasps_list[class_index][samp_index])\n",
    "    # Shuffle the data per class \n",
    "    for class_index in range(nb_classes): \n",
    "        random.shuffle(equalized_grasps[class_index])\n",
    "    return equalized_grasps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d55e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_total_YCB_eq_data_seq(data_set_loc_str, nb_samples, nb_classes, joint_dist_lim, seq_len, nb_tot_samp_class, test_train_ratio, rand_seed, verbose): \n",
    "    if verbose >= 1:\n",
    "        print(\"Loading Data.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    non_zero_grasps_list = load_non_zero_fused_grasps(data_set_loc_str, nb_classes, joint_dist_lim)\n",
    "    zero_grasps_list = load_zero_fused_grasps_buckets(data_set_loc_str, nb_classes, joint_dist_lim, nb_tot_samp_class)\n",
    "    equalized_grasps = equalize_class_buckets(non_zero_grasps_list, zero_grasps_list, nb_tot_samp_class, nb_classes)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    if verbose >= 1:\n",
    "        print(\"Time taken to load data: \", end_time - start_time)\n",
    "        \n",
    "    inputs = [] \n",
    "    outputs = []\n",
    "    \n",
    "    # Parsing all buckets \n",
    "    for bucket_index in range(nb_classes):\n",
    "        bucket_offset = 0 \n",
    "        while (bucket_offset + seq_len - 1  < len(equalized_grasps[bucket_index])):\n",
    "            # Build the sequence \n",
    "            tmp_buffer = [] \n",
    "            for seq_index in range(seq_len): \n",
    "                tmp_buffer.append(equalized_grasps[bucket_index][bucket_offset + seq_index].copy())\n",
    "            \n",
    "            inputs.append(tmp_buffer.copy())\n",
    "            outputs.append(tf.one_hot(bucket_index, nb_classes, dtype=np.float64).numpy().tolist())\n",
    "            bucket_offset += seq_len \n",
    "            \n",
    "        \n",
    "    # Shuffling the data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_train_ratio, random_state=rand_seed)\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print(\"Size of Train set: \", len(X_train))\n",
    "        print(\"Size of Test set: \", len(X_test))\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699b151",
   "metadata": {},
   "source": [
    "Apllying the functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2a1614b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data.\n",
      "Time taken to load data:  125.25978779792786\n",
      "Size of Train set:  25600\n",
      "Size of Test set:  6400\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_total_YCB_eq_data_seq(data_set_loc_str, nb_samples, nb_classes, joint_dist_lim, seq_len, nb_tot_samp_class, test_train_ratio, rand_seed, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32deb68b",
   "metadata": {},
   "source": [
    "Saving the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bfe8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(data_save_str_train, inputs = X_train, outputs = y_train)\n",
    "np.savez_compressed(data_save_str_test, inputs = X_test, outputs = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd538a",
   "metadata": {},
   "source": [
    "## Loading/verifying the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33dbfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_YCB_data_loc = data_save_str_train\n",
    "test_YCB_data_loc = data_save_str_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d952ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data_train = np.load(train_YCB_data_loc)\n",
    "X_train = file_data_train[\"inputs\"].astype(np.float64)\n",
    "y_train = file_data_train[\"outputs\"].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbe4d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data_test = np.load(test_YCB_data_loc)\n",
    "X_test = file_data_test[\"inputs\"].astype(np.float64)\n",
    "y_test = file_data_test[\"outputs\"].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e808e3b",
   "metadata": {},
   "source": [
    "# Non-zero only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "511ccc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_total_YCB_eq_data_seq_nz(data_set_loc_str, nb_samples, nb_classes, joint_dist_lim, seq_len, test_train_ratio, rand_seed, verbose): \n",
    "    if verbose >= 1:\n",
    "        print(\"Loading Data.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    non_zero_grasps_list = load_non_zero_fused_grasps(data_set_loc_str, nb_classes, joint_dist_lim)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    if verbose >= 1:\n",
    "        print(\"Time taken to load data: \", end_time - start_time)\n",
    "        \n",
    "    inputs = [] \n",
    "    outputs = []\n",
    "    \n",
    "    # Parsing all buckets \n",
    "    for bucket_index in range(nb_classes):\n",
    "        bucket_offset = 0 \n",
    "        while (bucket_offset + seq_len - 1  < len(non_zero_grasps_list[bucket_index])):\n",
    "            # Build the sequence \n",
    "            tmp_buffer = [] \n",
    "            for seq_index in range(seq_len): \n",
    "                tmp_buffer.append(non_zero_grasps_list[bucket_index][bucket_offset + seq_index].copy())\n",
    "            \n",
    "            inputs.append(tmp_buffer.copy())\n",
    "            outputs.append(tf.one_hot(bucket_index, nb_classes, dtype=np.float64).numpy().tolist())\n",
    "            bucket_offset += seq_len \n",
    "            \n",
    "        \n",
    "    # Shuffling the data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_train_ratio, random_state=rand_seed)\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print(\"Size of Train set: \", len(X_train))\n",
    "        print(\"Size of Test set: \", len(X_test))\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d1839780",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_nz = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f20c3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_str_train_nz = prefix_str + \"/non_zero_only/seq_len_\" + str(seq_len_nz) + \"/YCB_train.npz\"\n",
    "data_save_str_test_nz = prefix_str + \"/non_zero_only/seq_len_\" + str(seq_len_nz) + \"/YCB_test.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3457090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data.\n",
      "Time taken to load data:  105.6258454322815\n",
      "Size of Train set:  5882\n",
      "Size of Test set:  1471\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_total_YCB_eq_data_seq_nz(data_set_loc_str, nb_samples, nb_classes, joint_dist_lim, seq_len, test_train_ratio, rand_seed, verbose) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a77b8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(data_save_str_train_nz, inputs = X_train, outputs = y_train)\n",
    "np.savez_compressed(data_save_str_test_nz, inputs = X_test, outputs = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204a302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
